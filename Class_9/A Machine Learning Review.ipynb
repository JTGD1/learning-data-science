{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## A Machine Learning Review\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the course of the semester we have discussed a lot of different data science techniques and explored a lot of Python code for putting these concepts into use. The goal of this notebook is to provide a review of some of these concepts while making an effort at tying them together. We will also review some of the Python code we used to apply everything we've learned to some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A (Quick) Discussion on Python and Package Control\n",
    "Python makes use of *many* packages to do a wide range of tasks. Some of these packages are maintained by the same people that work on the Python programming language. Others are created by 3rd party teams. There are packages to do basic tasks like simple math and telling time. Other packages are used mainly for handling data, to do scientific computing, or machine learning. When doing homework assignments, we generally would only focus on 2 or 3 packages (usually with helpful hints!). In this notebook, we are going to introduce many new packages that may be useful for your final projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Structures\n",
    "In addition to storing strings, integers, and decimal (floats) numbers in Python, we have been using two main data structures: Python *lists* and *dictionaries*. I want to explain the difference between these two very briefly.\n",
    "\n",
    "Lists and dictionaries are both key-value stores: given a key (location) you can lookup a value. A list uses ordered keys to retrieve values. A dictionary uses unordered keys to return values. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_list = [5, 2, 6, 1, 7]\n",
    "my_list_2 = ['bob', 'natalia', 'panos', 'michelle', 'foster']\n",
    "print \"First list: %s\" % str(my_list)\n",
    "print \"Second list: %s\" % str(my_list_2)\n",
    "print \"The first (0'th) element in the first list is: %d\" % my_list[0]\n",
    "print \"The first (0'th) element in the second list is: %s\" % my_list_2[0]\n",
    "print \"The last element in the first list is : %d\" % my_list[-1]\n",
    "print \"The last element in the second list is : %s\" % my_list_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dictionary = {'bob': 5, 'natalia': 2, 'panos': 6, 'michelle': 1, 'foster': 7}\n",
    "print my_dictionary\n",
    "print \"The value for 'bob' is %d\" % my_dictionary['bob']\n",
    "print \"The value for 'panos' is %d\" % my_dictionary['panos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the list printed in order while the dictionary did not! I **can not** refer to the first item of a dictionary. There is no order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Packages\n",
    "This is not a comprehensive list, but is a good place to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. General Use\n",
    "These packages are used mainly to coordinate and structure your Python code. You can use `time` and `datetime` to keep track of how long it takes to run certain tasks or to format dates and times. The `os` and `sys` packages let you make calls to the computer and access programs outside of Python (e.g. the command line!). You can use `math` to do mathematical operations slightly more advanced than addition, subtraction, etc. (e.g. exponentiation). The `re` package lets you use regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Data Handling\n",
    "Python comes with packages for reading `csv`, `json`, and `xml` files natively. If you want to use something with more features, `pandas` is useful for creating data frames (a common data structure used in data science and machine learning). Some of you may be dealing with HTML data from web pages and will find Beautiful Soup 4 (`bs4`) and `urllib2` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import xml\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that when I imported pandas I decided to call it `pd`. This isn't necessary but is commonly used to give long packages a shorter name so that typing and reading them is easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Scientific Computing\n",
    "The `numpy` and `scipy` packages are probably two of the most popular Python packages in data science. They will give you the ability to use arrays and matrices (both dense and sparse). They also give a ton of basic operations (max, min, argmax, argmin, etc.) For those of you with Matlab experience, you may notice a lot of similarity as scipy and numpy were written based on Matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many people have asked why I use `np.max([1,2,3,4])` instead of just using Python's default function, `max([1,2,3,4])`. The answer is... I just happened to use the numpy version :) You can use whichever you like. In general, numpy has more math functions, so I usually just use it for all of them for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Machine Learning\n",
    "The package we have been using all semester to do machine learning, sci-kit learn (`sklearn`), is one of the most popular machine learning packages currently in use. Throughout the semeseter you have probably noticed that we have been using a *ton* of difference functions and features. The documentation on sklearn is vast. There are other machine learning packages, some of which only specialize in some small subset of tasks (e.g. only doing SVM, or only doing clustering). We won't dicuss any now, but if you are doing one thing, and need to tweak performance, you might find a dedicated package to be more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have also noticed that I often do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print \"The accuracy is %.2f\" % metrics.accuracy_score([1,1], [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I could also do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print \"The accuracy is %.2f\" % accuracy_score([1,1], [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no correct way of doing it. It's just a matter of preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for simplicity in lecture, I'm going to set a random seed so that we all get the same answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data Science Workflow\n",
    "&nbsp;\n",
    "<div style=\"float: left; width: 50%\">\n",
    "We've talked about the \"data science workflow\" a lot through out the semeseter, but I just want to remind everyone of what it looks like.\n",
    "\n",
    "<ol style=\"padding: 20px 0;\">\n",
    "<li>Business understanding</li>\n",
    "<li>Data understanding</li>\n",
    "<li>Data Preparation</li>\n",
    "<li>Modeling</li>\n",
    "<li>Evaluation</li>\n",
    "<li>Deployment</li>\n",
    "</ol>\n",
    "\n",
    "While you have been working a lot on the business understanding phase of your project recently (as well as the others, I hope!), today we are going to focus a bit more on summarizing what handson skills you have learned.\n",
    "</div>\n",
    "<div style=\"float: left; width: 40%\">\n",
    "<img src=\"images/workflow.png\" width=\"100%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Cleaning\n",
    "We've talked about this in our very first class and went on to mention it a few more times throughout the semester. However, I'd like to review some of this again given some common questions I've been getting.\n",
    "\n",
    "### 3.1. Structured Data\n",
    "Almost all of the data we have dealt with so far can be called *structured* data. This means that every record in the data set is organized and structured in some machine readable way. The three most popular ways of storing structured data are:\n",
    "\n",
    "- **.csv or .tsv** - Can be thought of as rows and columns, where each column will represent a single feature. All rows must have something for each column.\n",
    "- **JSON** - Looks similar to Python dictionaries. Each row can have an unordered list of `key:value`s\n",
    "- **XML** - Tagging language that looks something like HTML. More similar to JSON than to csv or tsv.\n",
    "\n",
    "The layout of any of these data types might seem straight forward, but there can be tons of complications. A file ending in `.csv` does *not* mean that it will be well structured. It is still just a text file.\n",
    "\n",
    "There are a series of data files in the `/data/` folder called `strings_*.csv`. These are csv files with no header. However, we know the columns are: 'age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head data/strings_ugly.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/strings_ugly.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look anywhere even close to what it should be. We can explicitely tell it to expect our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is going to kill the notebook :(\n",
    "# data = pd.read_csv(\"data/strings_ugly.csv\", names=['age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'])\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, that can't be right. If you look at the data you'll see that there are commas in one of the fields. Encapsulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head data/strings_quoted.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will also kill the notebook :(\n",
    "# data = pd.read_csv(\"data/strings_quoted.csv\", names=['age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'], \n",
    "#                    quotechar=\"\\\"\")\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting closer, but it looks like we also have quotes in the string. We have to escape them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head data/strings_escaped.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/strings_escaped.csv\", names=['age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'], quotechar=\"\\\"\", escapechar=\"\\\\\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can go on for a very long time until you find all the small nuances to your data file. Notice that we keep adding levels of complexity to our parser. Doing this at the command line is very tricky, which is why using pandas and `read_csv()` are very nice. A lot of the problems we just saw are unfortunately solved by editing the raw data to conform to some kind of standards. Hopefully, most of your project data is already in a useable state! Otherwise, regular expressions will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have three fields that aren't numeric. Since we need numeric features for all of our machine learning algorithms, let's convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert 'satisfaction' to be on a scale from -2 to +2\n",
    "data['satisfaction'] = data['satisfaction'].replace(['very low', 'low', 'neutral', 'high', 'very high'], \n",
    "                                                    [-2, -1, 0, 1, 2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can convert location into dummy variables by binarizing:\n",
    "for value in np.unique(data['location'])[0:-1]:\n",
    "    data['location_' + value] = pd.Series(data['location'] == value, dtype=int)\n",
    "data = data.drop(['location'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Last up is our text data, let's using a binary vectorizer to conver these to numeric\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the vectorizer\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "binary_vectorizer.fit(data['bio'])\n",
    "\n",
    "# I want to convert the numeric text data into a pandas DataFrame with meaninful column names. \n",
    "# To do this, I need to create a list of column headers. I will parse through the output of the\n",
    "# vocabulary to figure out the order and set column names.\n",
    "vocabulary = binary_vectorizer.vocabulary_\n",
    "bv_columns = range(len(vocabulary))\n",
    "for word in vocabulary:\n",
    "    bv_columns[vocabulary[word]] = \"bio_\" + word\n",
    "\n",
    "# Transform the data using the vectorizer, convert it to dense, put it into pandas with our column names\n",
    "bio_numeric = pd.DataFrame(binary_vectorizer.transform(data['bio']).todense(), columns=bv_columns)\n",
    "\n",
    "# Merge the data DataFrame and the text DataFrame\n",
    "data = pd.concat([data, bio_numeric], axis=1)\n",
    "\n",
    "# Drop the raw string data entirely\n",
    "data = data.drop(['bio'], axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not required, you will often see people place their target variable into a Python variable called `Y` and to put all their predictors into `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop(['purchased'], axis=1)\n",
    "Y = data['purchased']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Unstructured Data\n",
    "We've never really talked much about this in class, but some of you will have unstructured data in your projects. For example, web pages are a jumble of HTML tags. The formats between pages can be drastically different. In Homework 2 you used regular expressions to find pieces of an HTML page. However, we can also use BeautifulSoup to parse out the products for sale on this Etsy page: https://www.etsy.com/search?q=lamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the page and soupify it\n",
    "page = urllib2.urlopen('https://www.etsy.com/search?q=lamp')\n",
    "soup = bs4.BeautifulSoup(page.read(), \"html.parser\")\n",
    "\n",
    "# We will collect the title, seller, and prie of each product\n",
    "# To figure out where these are on the page, we need to do some manual exploration\n",
    "items = {'title': [], 'seller': [], 'price': []}\n",
    "for title, seller, price in zip(soup.findAll('a',{'class':'card-title'}), \n",
    "                                soup.findAll('a',{'class':'card-shop-name'}), \n",
    "                                soup.findAll('div',{'class':'card-price'})):\n",
    "    items['title'].append(title.text.strip())\n",
    "    items['seller'].append(seller.text.strip())\n",
    "    items['price'].append(price.text.replace('USD', '').strip())\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "etsy_items = pd.DataFrame(items)\n",
    "\n",
    "etsy_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only one small example. If you need some advice for your projects, feel free to ask me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "We've covered two different methods of modeling: supervised and unsupervised learning.\n",
    "\n",
    "### 4.1. Supervised\n",
    "Most of what we've done so far this semester involves having **labeled** data. For these data, we have a set of records where we know the value of the target variable. This allows us to learn some relationship between our feature set and the target variable. We've covered five machine learning algorithms that can do this. Here is a brief, and in no way comprehensive, overview.\n",
    "\n",
    "<table>\n",
    "<tr><td>Model</td>\n",
    "<td>Overview</td>\n",
    "<td>Pros</td>\n",
    "<td>Cons</td>\n",
    "<td>Use Case</td></tr>\n",
    "\n",
    "<tr><td>Tree Structured</td>\n",
    " <td>Will create splits on any feature that gives maximum **information gain**.</td>\n",
    " <td>- Non-linear model (low bias) <br />\n",
    "     - (specifically) Can arbitrarily carve up example space into \"rectangular\" regions<br />\n",
    "     - Fast test for non-linearity in a data set</td>\n",
    " <td>- Separating planes will be perpendicular to a feature<br />\n",
    "     - Prone to overfitting</td>\n",
    " <td>- Data with moderate numbers of (relevant) mixed numeric and categorical features</td></tr>\n",
    " \n",
    "<tr><td>Logistic Regression</td>\n",
    " <td>Creates a hyperplane that can separate the data with the smallest **loss**.</td>\n",
    " <td>- Coefficients to interpret<br />\n",
    "     - Low overfitting (low variance)</td>\n",
    " <td>- Coefficients to interpret<br />\n",
    "     - No closed form solution<br />\n",
    "     - Can be slow (need to think about optimization routine \"under the hood\")<br />\n",
    "     - Will only learn \"linear part\" of true concept (high bias)</td>\n",
    " <td>- Always try it</td></tr>\n",
    "\n",
    "\n",
    "<tr><td>SVM</td>\n",
    " <td>Creates a hyperplane that can separate the data with the maximal **margin**.</td>\n",
    " <td>- Different \"kernels\" available</td>\n",
    " <td>- No closed form solution<br />\n",
    "     - Can be sloooooow </td>\n",
    " <td>- Text data</td></tr>\n",
    "\n",
    "<tr><td>Naive Bayes</td>\n",
    " <td>Uses simple counts to calculate **conditional probabilities**.</td>\n",
    " <td>- Fast training<br />\n",
    "     - Can be implemented with SQL queries (or in Excel!)</td>\n",
    " <td>- Treats all features as independent</td>\n",
    " <td>- Text data</td></tr>\n",
    "\n",
    "<tr><td>k-NN</td>\n",
    " <td>Creates a **cluster** of the $k$-closest records and combines their labels (e.g., with majority voting or an average).</td>\n",
    " <td>- Works with any number of labels<br />\n",
    "     - Very fast \"learning\" (lazy)</td>\n",
    " <td>- Very slow prediction</td>\n",
    " <td>- When choosing closest cases makes sense to the users/stakeholders</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Unsupervised\n",
    "Unsupervised algorithms are used for data where there is no target variable or no labels for your target variable. We only started covering these recently.\n",
    "\n",
    "<table>\n",
    "<tr><td>Model</td>\n",
    "<td>Overview</td>\n",
    "<td>Pros</td>\n",
    "<td>Cons</td></tr>\n",
    "\n",
    "<tr><td>K-Means</td>\n",
    " <td>Creates **$k$ clusters** where each record belongs to the cluster with the closest mean (center)</td>\n",
    " <td>- Fast (relatively)</td>\n",
    " <td>- k is very likely unknown<br />\n",
    "     - Nondeterministic</td></tr>\n",
    " \n",
    "<tr><td>Hierarchical Clustering</td>\n",
    " <td>Creates an increasing number of clusters by continually **merging clusters** that are closest together (clusters can be single records)</td>\n",
    " <td>- The number of clusters does not need to be preset</td>\n",
    " <td></td></tr>\n",
    "\n",
    "<!--<tr><td>Dimensionality Reduction</td>\n",
    " <td>Takes a set of records with $M$ features and reduces it to the top $m$ features (that explain the most variance), where $m < M$.</td>\n",
    " <td></td>\n",
    " <td></td></tr>-->\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementation\n",
    "All of these algorithms have an implementation in sklearn. Some algorithms, like SVM, have multiple implementations. Let's import one implementation of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given you have imported your model, the general process for using the model is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X, Y) # there is no equal sign here!\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't matter what model you are using, it is always the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, Y) # there is no equal sign here!\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assessing\n",
    "How do we know if our model is any good? There are many ways of doing this! It depends on your particular use case.  Here are the two metrics we talked about most for classification/ranking. \n",
    "\n",
    "<table>\n",
    "<tr><td>Metric</td>\n",
    " <td>Overview</td>\n",
    " <td>Pros</td>\n",
    " <td>Cons</td></tr>\n",
    "<tr><td>Accuracy</td>\n",
    " <td>The percentage of things you got correct.</td>\n",
    " <td>- Easy to calculate and interpret</td>\n",
    " <td>- Doesn't account for business costs<br />\n",
    " - Doesn't account for baseline</td></tr>\n",
    "<tr><td>ROC/AUC</td>\n",
    " <td>False positive rate vs. True positive rate.</td>\n",
    " <td>- Allows for fine grained assessment<br />\n",
    " - Deals with skew</td>\n",
    " <td>- Can be difficult to understand<br />\n",
    " - Exploring multiple ROC curves can become messy</td></tr>\n",
    "</table>\n",
    "\n",
    "We spoke of other metrics that I'm not discussing here: lift, precision, recall, mean squared error, etc.\n",
    "\n",
    "Accuracy, ROC curves, and area under the ROC curve calculations (as well as many others) are straight forward in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)\n",
    "\n",
    "print \"The accuracy is %.3f\" % accuracy_score(Y, prediction)\n",
    "print \"The AUC is %.3f\" % roc_auc_score(Y, probabilities[:, 1])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y, probabilities[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Assessing II (splitting)\n",
    "What are the downsides so training and predicting on the same data? Overfitting! We know two ways to work around this: (1) train/test splitting, and (2) cross validation. Both of these things are, again, built into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "prediction = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "\n",
    "print \"The accuracy is %.3f\" % accuracy_score(Y_test, prediction)\n",
    "print \"The AUC is %.3f\" % roc_auc_score(Y_test, probabilities[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `cross_val_score()` function, we can assess models using cross validation in only one line. Check the [documentation](http://scikit-learn.org/stable/modules/model_evaluation.html) for a list of possible `scoring` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "model = LogisticRegression()\n",
    "\n",
    "print \"The accuracy is %.3f\" % np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"accuracy\"))\n",
    "print \"The AUC is %.3f\" % np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"roc_auc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tuning and Complexity\n",
    "By default, all the models we use in sklearn have some settings that manage how complex they are. We've learned quite a few of these complexity parameters (usually called hyper parameters) already. The act of finding the \"best\" parameter is usually done through the act of \"hyper parameter tuning\".\n",
    "\n",
    "<table>\n",
    "<tr><td>Model Type</td>\n",
    " <td>Parameter</td>\n",
    " <td>Explanation</td>\n",
    " <td>Good Range</td></tr>\n",
    "\n",
    "<tr><td>Tree</td>\n",
    " <td>max_depth</td>\n",
    " <td>Maximum number of levels to build</td>\n",
    " <td>[1, log<sub>2</sub>(# records)]</td></tr>\n",
    "\n",
    "<tr><td>Tree</td>\n",
    " <td>min_samples_split</td>\n",
    " <td>Minimum number of records that must be in a node for it to be split.</td>\n",
    " <td>[1, # records]</td></tr>\n",
    " \n",
    "<tr><td>Tree</td>\n",
    " <td>min_samples_leaf</td>\n",
    " <td>Minimum number of records that must be at a node to call it a leaf.</td>\n",
    " <td>[1, # records]</td></tr>\n",
    "\n",
    "<tr><td>LR</td>\n",
    " <td>C</td>\n",
    " <td>Regularization parameter. How heavily should the model be penalized for being complex?</td>\n",
    " <td>[10<sup>-10</sup>, 10<sup>10</sup>]</td></tr>\n",
    "\n",
    "<tr><td>SVM</td>\n",
    " <td>C</td>\n",
    " <td>Similar to logistic regression</td>\n",
    " <td>[10<sup>-10</sup>, 10<sup>10</sup>]</td></tr>\n",
    " \n",
    "<tr><td>NB</td>\n",
    " <td>alpha</td>\n",
    " <td>Smoothing constant. Essential to ensure 0 probabilities don't zero-out the product.  Used also to keep small counts from making a big difference.</td>\n",
    " <td>[0, ...]</td></tr>\n",
    " \n",
    "<tr><td>k-NN</td>\n",
    " <td>k</td>\n",
    " <td>Number of neighbors, usually odd to avoid ties</td>\n",
    " <td>[1, number_records]</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hyper_parameters = range(-10, 11)\n",
    "accuracies = []\n",
    "aucs = []\n",
    "\n",
    "for hyper_parameter in hyper_parameters:\n",
    "    c = np.power(10.0, hyper_parameter)\n",
    "    \n",
    "    model = LogisticRegression(C=c)\n",
    "    \n",
    "    accuracies.append(np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"accuracy\")))\n",
    "    aucs.append(np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"roc_auc\")))\n",
    "\n",
    "print \"Maximum accuracy is %.3f and occured with parameter setting of %.3e\" % (np.max(accuracies), hyper_parameters[np.argmax(accuracies)])\n",
    "\n",
    "print \"Maximum AUC is %.3f and occured with parameter setting of %.3e\" % (np.max(aucs), hyper_parameters[np.argmax(aucs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
