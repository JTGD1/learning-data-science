{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## A Machine Learning Review\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the course of the semester we have discussed a lot of different data science techniques and explored a lot of Python code for putting these concepts into use. The goal of this notebook is to provide a review of some of these concepts while making an effort at tying them together. We will also review some of the Python code we used to apply everything we've learned to some real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A (Quick) Discussion on Python and Package Control\n",
    "Python makes use of *many* packages to do a wide range of tasks. Some of these packages are maintained by the same people that work on the Python programming languages. Others are created by 3rd party teams. There are packages to do basic tasks like simple math and telling time. Other packages are used mainly for handling data, to do scientific computing, or machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Structures\n",
    "In addition to storing strings, integers, and decimal (floats) numbers in Python, we have been using two main data structures: Python *lists* and *dictionaries*. I want to explain the difference between these two very briefly.\n",
    "\n",
    "Lists and dictionaries are both key-value stores: given a key (location) you can recieve a value. A list uses ordered keys to retrieve values. A dictionary uses unordered keys to return values. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First list: [5, 2, 6, 1, 7]\n",
      "Second list: ['bob', 'natalia', 'panos', 'michelle', 'foster']\n",
      "The first (0'th) element in the first list is: 5\n",
      "The first (0'th) element in the second list is: bob\n",
      "The last element in the first list is : 7\n",
      "The last element in the second list is : foster\n"
     ]
    }
   ],
   "source": [
    "my_list = [5, 2, 6, 1, 7]\n",
    "my_list_2 = ['bob', 'natalia', 'panos', 'michelle', 'foster']\n",
    "print \"First list: %s\" % str(my_list)\n",
    "print \"Second list: %s\" % str(my_list_2)\n",
    "print \"The first (0'th) element in the first list is: %d\" % my_list[0]\n",
    "print \"The first (0'th) element in the second list is: %s\" % my_list_2[0]\n",
    "print \"The last element in the first list is : %d\" % my_list[-1]\n",
    "print \"The last element in the second list is : %s\" % my_list_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'michelle': 1, 'bob': 5, 'foster': 7, 'panos': 6, 'natalia': 2}\n",
      "The value for 'bob' is 5\n",
      "The value for 'panos' is 6\n"
     ]
    }
   ],
   "source": [
    "my_dictionary = {'bob': 5, 'natalia': 2, 'panos': 6, 'michelle': 1, 'foster': 7}\n",
    "print my_dictionary\n",
    "print \"The value for 'bob' is %d\" % my_dictionary['bob']\n",
    "print \"The value for 'panos' is %d\" % my_dictionary['panos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the list printed in order while the dictionary did not! I **can not** refer to the first item of a dictionary. There is no order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. General Use\n",
    "These packages are used mainly to coordinate and structure your Python code. You can use `time` and `datetime` to keep track of how long it takes to run certain tasks or to format dates and times. The `os` and `sys` packages let you make calls to the computer and access programs outside of Python (e.g. the command line!). You can use `math` to do mathematical operations slightly more advanced than addition, subtraction, etc. (e.g. exponentiation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Data Handling\n",
    "Python comes with packages for reading `csv` and `json` files natively. If you want to use something with more features, `pandas` is useful for creating data frames (a common data structure used in data science and machine learning). Some of you may be dealing with HTML data from web pages and will find Beautiful Soup 4 (`bs4`) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "# import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that when I imported pandas I decided to call it `pd`. This isn't necessary but is commonly used to give long packages a shorter name so that typing and reading them is easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Scientific Computing\n",
    "The `numpy` and `scipy` packages are probably two of the most popular Python packages. They will give you the ability to use arrays and matrices (both dense and sparse). They also give a ton of basic operations (max, min, argmax, argmin, etc.) For those of you with Matlab experience, you may notice a lot of similarity as scipy and numpy were written based on Matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many people have asked why I use `np.max([1,2,3,4])` instead of just using Python's default function, `max([1,2,3,4])`. The answer is... I just happened to use the numpy version :) You can use whichever you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Machine Learning\n",
    "The package we have been using all semester to do machine learning, sci-kit learn (`sklearn`), is one of the most popular machine learning packages currently in use. Throughout the semeseter you have probably noticed that we have been using a *ton* of difference functions and features. The documentation on sklearn is vast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have also noticed that I often do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print \"The accuracy is %.2f\" % metrics.accuracy_score([1,1], [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I could also do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print \"The accuracy is %.2f\" % accuracy_score([1,1], [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no correct way of doing it. It's just a matter of preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data Science Workflow\n",
    "&nbsp;\n",
    "<div style=\"float: left; width: 50%\">\n",
    "We've talked about the \"data science workflow\" a lot through out the semeseter, but I just want to remind everyone of what it looks like.\n",
    "\n",
    "<ol style=\"padding: 20px 0;\">\n",
    "<li>Business understanding</li>\n",
    "<li>Data understanding</li>\n",
    "<li>Data Preparation</li>\n",
    "<li>Modeling</li>\n",
    "<li>Evaluation</li>\n",
    "<li>Deployment</li>\n",
    "</ol>\n",
    "\n",
    "While you have been working a lot on the business understanding phase of your project recently (as well as the others, I hope!), today we are going to focus a bit more on summarizing what handson skills you have learned.\n",
    "</div>\n",
    "<div style=\"float: left; width: 40%\">\n",
    "<img src=\"images/workflow.png\" width=\"100%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Cleaning\n",
    "We've talked about this in our very first class and went on to mention it a few more times throughout the semester. However, I'd like to review some of this again given some common questions I've been getting.\n",
    "\n",
    "### 3.1. Structured Data\n",
    "Almost all of the data we have dealt with so far can be called *structured* data. This means that every record in the data set is organized and structured in some machine readable way. The three most popular ways of storing structured data are:\n",
    "\n",
    "- **.csv or .tsv** - Can be thought of as rows and columns, where each column will represent a single feature. All rows must have something for each column.\n",
    "- **JSON** - Looks similar to Python dictionaries. Each row can have an unordered list of `key:value`s\n",
    "- **XML**\n",
    "\n",
    "The layout of any of these data types might seem straight forward, but there can be tons of complications. A file ending in `.csv` does *not* mean that it will be well structured. It is still just a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0,neutral,eu,3.952074059758619,-19.48620784914078,Sodales \"vivamus\" in, risus molestie, egestas in.,0\r\n",
      "28.0,neutral,sa,3.5295183836057595,51.284180040232215,Pellentesque arcu sed.,1\r\n",
      "37.0,high,sa,4.254526317975149,97.34526006557826,Neque odio, in nulla, lorem nec.,0\r\n",
      "42.0,high,sa,4.924077485580787,80.24260604790156,Lorem non pretium.,0\r\n",
      "56.0,high,af,6.436250132712625,42.78962533750958,Sem dictum dolor.,0\r\n",
      "40.0,neutral,af,4.576757605316351,-1.0876572412988317,Neque condimentum.,1\r\n",
      "69.0,neutral,eu,5.365851342999525,-15.770934329395772,Nisl fames ipsum, amet laoreet.,0\r\n",
      "44.0,high,sa,2.912293368604344,73.85944600120466,Arcu quisque, vitae turpis integer, fusce luctus.,1\r\n",
      "63.0,neutral,eu,4.376757476733249,3.9510213482794034,Feugiat diam, at ipsum.,0\r\n",
      "56.0,neutral,eu,3.461138913269333,-46.426105926443086,Metus elit.,1\r\n"
     ]
    }
   ],
   "source": [
    "!head data/strings_ugly.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>63.0</th>\n",
       "      <th>neutral</th>\n",
       "      <th>eu</th>\n",
       "      <th>3.952074059758619</th>\n",
       "      <th>-19.48620784914078</th>\n",
       "      <th>Sodales \"vivamus\" in</th>\n",
       "      <th> risus molestie</th>\n",
       "      <th> egestas in.</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 28</td>\n",
       "      <td> neutral</td>\n",
       "      <td> sa</td>\n",
       "      <td> 3.529518</td>\n",
       "      <td> 51.284180</td>\n",
       "      <td> Pellentesque arcu sed.</td>\n",
       "      <td>         1</td>\n",
       "      <td>         NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 37</td>\n",
       "      <td>    high</td>\n",
       "      <td> sa</td>\n",
       "      <td> 4.254526</td>\n",
       "      <td> 97.345260</td>\n",
       "      <td>             Neque odio</td>\n",
       "      <td>  in nulla</td>\n",
       "      <td>  lorem nec.</td>\n",
       "      <td>  0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 42</td>\n",
       "      <td>    high</td>\n",
       "      <td> sa</td>\n",
       "      <td> 4.924077</td>\n",
       "      <td> 80.242606</td>\n",
       "      <td>     Lorem non pretium.</td>\n",
       "      <td>         0</td>\n",
       "      <td>         NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 56</td>\n",
       "      <td>    high</td>\n",
       "      <td> af</td>\n",
       "      <td> 6.436250</td>\n",
       "      <td> 42.789625</td>\n",
       "      <td>      Sem dictum dolor.</td>\n",
       "      <td>         0</td>\n",
       "      <td>         NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 40</td>\n",
       "      <td> neutral</td>\n",
       "      <td> af</td>\n",
       "      <td> 4.576758</td>\n",
       "      <td> -1.087657</td>\n",
       "      <td>     Neque condimentum.</td>\n",
       "      <td>         1</td>\n",
       "      <td>         NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   63.0  neutral  eu  3.952074059758619  -19.48620784914078  \\\n",
       "0    28  neutral  sa           3.529518           51.284180   \n",
       "1    37     high  sa           4.254526           97.345260   \n",
       "2    42     high  sa           4.924077           80.242606   \n",
       "3    56     high  af           6.436250           42.789625   \n",
       "4    40  neutral  af           4.576758           -1.087657   \n",
       "\n",
       "     Sodales \"vivamus\" in  risus molestie  egestas in.   0  \n",
       "0  Pellentesque arcu sed.               1          NaN NaN  \n",
       "1              Neque odio        in nulla   lorem nec.   0  \n",
       "2      Lorem non pretium.               0          NaN NaN  \n",
       "3       Sem dictum dolor.               0          NaN NaN  \n",
       "4      Neque condimentum.               1          NaN NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/strings_ugly.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look anywhere even close to what it should be. We can explicitely tell it to expect our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is going to kill the notebook :(\n",
    "# data = pd.read_csv(\"data/strings_ugly.csv\", names=['age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'])\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, that can't be right. If you look at the data you'll see that there are commas in one of the fields. Encapsulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0,neutral,eu,3.952074059758619,-19.48620784914078,\"Sodales \"vivamus in, risus molestie, egestas in.\",0\r\n",
      "28.0,neutral,sa,3.5295183836057595,51.284180040232215,Pellentesque arcu sed.,1\r\n",
      "37.0,high,sa,4.254526317975149,97.34526006557826,\"Neque odio, in nulla, lorem nec.\",0\r\n",
      "42.0,high,sa,4.924077485580787,80.24260604790156,Lorem non pretium.,0\r\n",
      "56.0,high,af,6.436250132712625,42.78962533750958,Sem dictum dolor.,0\r\n",
      "40.0,neutral,af,4.576757605316351,-1.0876572412988317,Neque condimentum.,1\r\n",
      "69.0,neutral,eu,5.365851342999525,-15.770934329395772,\"Nisl fames ipsum, amet laoreet.\",0\r\n",
      "44.0,high,sa,2.912293368604344,73.85944600120466,\"Arcu quisque, vitae turpis integer, fusce luctus.\",1\r\n",
      "63.0,neutral,eu,4.376757476733249,3.9510213482794034,\"Feugiat diam, at ipsum.\",0\r\n",
      "56.0,neutral,eu,3.461138913269333,-46.426105926443086,Metus elit.,1\r\n"
     ]
    }
   ],
   "source": [
    "!head data/strings_quoted.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will also kill the notebook :(\n",
    "# data = pd.read_csv(\"data/strings_quoted.csv\", names=['age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'], \n",
    "#                    quotechar=\"\\\"\")\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting closer, but it looks like we also have quotes in the string. We have to escape them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0,neutral,eu,3.952074059758619,-19.48620784914078,\"Sodales \\\"vivamus\\\" in, risus molestie, egestas in.\",0\r\n",
      "28.0,neutral,sa,3.5295183836057595,51.284180040232215,Pellentesque arcu sed.,1\r\n",
      "37.0,high,sa,4.254526317975149,97.34526006557826,\"Neque odio, in nulla, lorem nec.\",0\r\n",
      "42.0,high,sa,4.924077485580787,80.24260604790156,Lorem non pretium.,0\r\n",
      "56.0,high,af,6.436250132712625,42.78962533750958,Sem dictum dolor.,0\r\n",
      "40.0,neutral,af,4.576757605316351,-1.0876572412988317,Neque condimentum.,1\r\n",
      "69.0,neutral,eu,5.365851342999525,-15.770934329395772,\"Nisl fames ipsum, amet laoreet.\",0\r\n",
      "44.0,high,sa,2.912293368604344,73.85944600120466,\"Arcu quisque, vitae turpis integer, fusce luctus.\",1\r\n",
      "63.0,neutral,eu,4.376757476733249,3.9510213482794034,\"Feugiat diam, at ipsum.\",0\r\n",
      "56.0,neutral,eu,3.461138913269333,-46.426105926443086,Metus elit.,1\r\n"
     ]
    }
   ],
   "source": [
    "!head data/strings_escaped.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>location</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>bio</th>\n",
       "      <th>purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 63</td>\n",
       "      <td> neutral</td>\n",
       "      <td> eu</td>\n",
       "      <td> 3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td> Sodales \"vivamus\" in, risus molestie, egestas in.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 28</td>\n",
       "      <td> neutral</td>\n",
       "      <td> sa</td>\n",
       "      <td> 3.529518</td>\n",
       "      <td> 51.284180</td>\n",
       "      <td>                            Pellentesque arcu sed.</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 37</td>\n",
       "      <td>    high</td>\n",
       "      <td> sa</td>\n",
       "      <td> 4.254526</td>\n",
       "      <td> 97.345260</td>\n",
       "      <td>                  Neque odio, in nulla, lorem nec.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 42</td>\n",
       "      <td>    high</td>\n",
       "      <td> sa</td>\n",
       "      <td> 4.924077</td>\n",
       "      <td> 80.242606</td>\n",
       "      <td>                                Lorem non pretium.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 56</td>\n",
       "      <td>    high</td>\n",
       "      <td> af</td>\n",
       "      <td> 6.436250</td>\n",
       "      <td> 42.789625</td>\n",
       "      <td>                                 Sem dictum dolor.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age satisfaction location  time_spent     income  \\\n",
       "0   63      neutral       eu    3.952074 -19.486208   \n",
       "1   28      neutral       sa    3.529518  51.284180   \n",
       "2   37         high       sa    4.254526  97.345260   \n",
       "3   42         high       sa    4.924077  80.242606   \n",
       "4   56         high       af    6.436250  42.789625   \n",
       "\n",
       "                                                 bio  purchased  \n",
       "0  Sodales \"vivamus\" in, risus molestie, egestas in.          0  \n",
       "1                             Pellentesque arcu sed.          1  \n",
       "2                   Neque odio, in nulla, lorem nec.          0  \n",
       "3                                 Lorem non pretium.          0  \n",
       "4                                  Sem dictum dolor.          0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/strings_escaped.csv\", names=['age', 'satisfaction', 'location', 'time_spent', 'income', 'bio', 'purchased'], quotechar=\"\\\"\", escapechar=\"\\\\\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can go on for a very long time until you find all the small nuances to your data file. Notice that we keep adding levels of complexity to our parser. Doing this at the command line is very tricky, which is why using pandas and `read_csv()` are very nice. A lot of the problems we just saw are unfortunately solved by editing the raw data to conform to some kind of standards. Hopefully, most of your project data is already in a useable state!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have three fields that aren't numeric. Since we need numeric features for all of our machine learning algorithms, let's convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>location</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>bio</th>\n",
       "      <th>purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 63</td>\n",
       "      <td> 0</td>\n",
       "      <td> eu</td>\n",
       "      <td> 3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td> Sodales \"vivamus\" in, risus molestie, egestas in.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 28</td>\n",
       "      <td> 0</td>\n",
       "      <td> sa</td>\n",
       "      <td> 3.529518</td>\n",
       "      <td> 51.284180</td>\n",
       "      <td>                            Pellentesque arcu sed.</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 37</td>\n",
       "      <td> 1</td>\n",
       "      <td> sa</td>\n",
       "      <td> 4.254526</td>\n",
       "      <td> 97.345260</td>\n",
       "      <td>                  Neque odio, in nulla, lorem nec.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 42</td>\n",
       "      <td> 1</td>\n",
       "      <td> sa</td>\n",
       "      <td> 4.924077</td>\n",
       "      <td> 80.242606</td>\n",
       "      <td>                                Lorem non pretium.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 56</td>\n",
       "      <td> 1</td>\n",
       "      <td> af</td>\n",
       "      <td> 6.436250</td>\n",
       "      <td> 42.789625</td>\n",
       "      <td>                                 Sem dictum dolor.</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  satisfaction location  time_spent     income  \\\n",
       "0   63             0       eu    3.952074 -19.486208   \n",
       "1   28             0       sa    3.529518  51.284180   \n",
       "2   37             1       sa    4.254526  97.345260   \n",
       "3   42             1       sa    4.924077  80.242606   \n",
       "4   56             1       af    6.436250  42.789625   \n",
       "\n",
       "                                                 bio  purchased  \n",
       "0  Sodales \"vivamus\" in, risus molestie, egestas in.          0  \n",
       "1                             Pellentesque arcu sed.          1  \n",
       "2                   Neque odio, in nulla, lorem nec.          0  \n",
       "3                                 Lorem non pretium.          0  \n",
       "4                                  Sem dictum dolor.          0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'satisfaction' to be on a scale from -2 to +2\n",
    "data['satisfaction'] = data['satisfaction'].replace(['very low', 'low', 'neutral', 'high', 'very high'], \n",
    "                                                    [-2, -1, 0, 1, 2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>bio</th>\n",
       "      <th>purchased</th>\n",
       "      <th>location_a</th>\n",
       "      <th>location_af</th>\n",
       "      <th>location_aus</th>\n",
       "      <th>location_eu</th>\n",
       "      <th>location_in</th>\n",
       "      <th>location_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 63</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td> Sodales \"vivamus\" in, risus molestie, egestas in.</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 28</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.529518</td>\n",
       "      <td> 51.284180</td>\n",
       "      <td>                            Pellentesque arcu sed.</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 37</td>\n",
       "      <td> 1</td>\n",
       "      <td> 4.254526</td>\n",
       "      <td> 97.345260</td>\n",
       "      <td>                  Neque odio, in nulla, lorem nec.</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 42</td>\n",
       "      <td> 1</td>\n",
       "      <td> 4.924077</td>\n",
       "      <td> 80.242606</td>\n",
       "      <td>                                Lorem non pretium.</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 56</td>\n",
       "      <td> 1</td>\n",
       "      <td> 6.436250</td>\n",
       "      <td> 42.789625</td>\n",
       "      <td>                                 Sem dictum dolor.</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  satisfaction  time_spent     income  \\\n",
       "0   63             0    3.952074 -19.486208   \n",
       "1   28             0    3.529518  51.284180   \n",
       "2   37             1    4.254526  97.345260   \n",
       "3   42             1    4.924077  80.242606   \n",
       "4   56             1    6.436250  42.789625   \n",
       "\n",
       "                                                 bio  purchased  location_a  \\\n",
       "0  Sodales \"vivamus\" in, risus molestie, egestas in.          0           0   \n",
       "1                             Pellentesque arcu sed.          1           0   \n",
       "2                   Neque odio, in nulla, lorem nec.          0           0   \n",
       "3                                 Lorem non pretium.          0           0   \n",
       "4                                  Sem dictum dolor.          0           0   \n",
       "\n",
       "   location_af  location_aus  location_eu  location_in  location_na  \n",
       "0            0             0            1            0            0  \n",
       "1            0             0            0            0            0  \n",
       "2            0             0            0            0            0  \n",
       "3            0             0            0            0            0  \n",
       "4            1             0            0            0            0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can convert location into dummy variables by binarizing:\n",
    "for value in np.unique(data['location'])[0:-1]:\n",
    "    data['location_' + value] = pd.Series(data['location'] == value, dtype=int)\n",
    "data = data.drop(['location'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Last up is our text data, let's using a binary vectorizer to conver these to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>income</th>\n",
       "      <th>purchased</th>\n",
       "      <th>location_a</th>\n",
       "      <th>location_af</th>\n",
       "      <th>location_aus</th>\n",
       "      <th>location_eu</th>\n",
       "      <th>location_in</th>\n",
       "      <th>...</th>\n",
       "      <th>bio_velit</th>\n",
       "      <th>bio_venenatis</th>\n",
       "      <th>bio_vestibulum</th>\n",
       "      <th>bio_vitae</th>\n",
       "      <th>bio_vivamus</th>\n",
       "      <th>bio_viverra</th>\n",
       "      <th>bio_voluptatem</th>\n",
       "      <th>bio_volutpat</th>\n",
       "      <th>bio_vulputate</th>\n",
       "      <th>bio_wisi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 63</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.952074</td>\n",
       "      <td>-19.486208</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 28</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3.529518</td>\n",
       "      <td> 51.284180</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 37</td>\n",
       "      <td> 1</td>\n",
       "      <td> 4.254526</td>\n",
       "      <td> 97.345260</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 42</td>\n",
       "      <td> 1</td>\n",
       "      <td> 4.924077</td>\n",
       "      <td> 80.242606</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 56</td>\n",
       "      <td> 1</td>\n",
       "      <td> 6.436250</td>\n",
       "      <td> 42.789625</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  satisfaction  time_spent     income  purchased  location_a  \\\n",
       "0   63             0    3.952074 -19.486208          0           0   \n",
       "1   28             0    3.529518  51.284180          1           0   \n",
       "2   37             1    4.254526  97.345260          0           0   \n",
       "3   42             1    4.924077  80.242606          0           0   \n",
       "4   56             1    6.436250  42.789625          0           0   \n",
       "\n",
       "   location_af  location_aus  location_eu  location_in    ...     bio_velit  \\\n",
       "0            0             0            1            0    ...             0   \n",
       "1            0             0            0            0    ...             0   \n",
       "2            0             0            0            0    ...             0   \n",
       "3            0             0            0            0    ...             0   \n",
       "4            1             0            0            0    ...             0   \n",
       "\n",
       "   bio_venenatis  bio_vestibulum  bio_vitae  bio_vivamus  bio_viverra  \\\n",
       "0              0               0          0            1            0   \n",
       "1              0               0          0            0            0   \n",
       "2              0               0          0            0            0   \n",
       "3              0               0          0            0            0   \n",
       "4              0               0          0            0            0   \n",
       "\n",
       "   bio_voluptatem  bio_volutpat  bio_vulputate  bio_wisi  \n",
       "0               0             0              0         0  \n",
       "1               0             0              0         0  \n",
       "2               0             0              0         0  \n",
       "3               0             0              0         0  \n",
       "4               0             0              0         0  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "binary_vectorizer.fit(data['bio'])\n",
    "\n",
    "vocabulary = binary_vectorizer.vocabulary_\n",
    "bv_columns = range(len(vocabulary))\n",
    "for word in vocabulary:\n",
    "    bv_columns[vocabulary[word]] = \"bio_\" + word\n",
    "    \n",
    "bio_numeric = pd.DataFrame(binary_vectorizer.transform(data['bio']).todense(), columns=bv_columns)\n",
    "data = pd.concat([data, bio_numeric], axis=1)\n",
    "data = data.drop(['bio'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not required, you will often see people place their target variable into a Python variable called `Y` and to put all their predictors into `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop(['purchased'], axis=1)\n",
    "Y = data['purchased']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Unstructured Data\n",
    "We've never really talked much about this in class, but some of you will have unstructured data in your projects. For example, web pages are a jumble of HTML tags. The formats between pages can be drastically different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "We've covered two different methods of modeling: supervised and unsupervised learning.\n",
    "\n",
    "### 4.1. Supervised\n",
    "Most of what we've done so far this semester involves having **labeled** data. For these data, we have a set of records where we know the value of the target variable. This allows us to learn some relationship between our feature set and the target variable. We've covered five machine learning algorithms that can do this. Here is a brief, and in no way comprehensive, overview.\n",
    "\n",
    "<table>\n",
    "<tr><td>Model</td>\n",
    "<td>Overview</td>\n",
    "<td>Pros</td>\n",
    "<td>Cons</td>\n",
    "<td>Use Case</td></tr>\n",
    "\n",
    "<tr><td>Tree Structured</td>\n",
    " <td>Will create splits on any feature that gives maximum **information gain**.</td>\n",
    " <td>- Non-linear model (low bias) <br />\n",
    "     - (specifically) Can arbitrarily carve up example space into \"rectangular\" regions<br />\n",
    "     - Fast test for non-linearity in a data set</td>\n",
    " <td>- Separating planes will be perpendicular to a feature<br />\n",
    "     - Prone to overfitting</td>\n",
    " <td>- Data with moderate numbers of (relevant) mixed numeric and categorical features</td></tr>\n",
    " \n",
    "<tr><td>Logistic Regression</td>\n",
    " <td>Creates a hyperplane that can separate the data with the smallest **loss**.</td>\n",
    " <td>- Coefficients to interpret<br />\n",
    "     - Low overfitting (low variance)</td>\n",
    " <td>- Coefficients to interpret<br />\n",
    "     - No closed form solution<br />\n",
    "     - Can be slow (need to think about optimization routine \"under the hood\")<br />\n",
    "     - Will only learn \"linear part\" of true concept (high bias)</td>\n",
    " <td>- Always try it</td></tr>\n",
    "\n",
    "\n",
    "<tr><td>SVM</td>\n",
    " <td>Creates a hyperplane that can separate the data with the maximal **margin**.</td>\n",
    " <td>- Different \"kernels\" available</td>\n",
    " <td>- No closed form solution<br />\n",
    "     - Can be sloooooow </td>\n",
    " <td>- Text data</td></tr>\n",
    "\n",
    "<tr><td>Naive Bayes</td>\n",
    " <td>Uses simple counts to calculate **conditional probabilities**.</td>\n",
    " <td>- Fast training<br />\n",
    "     - Can be implemented with SQL queries (or in Excel!)</td>\n",
    " <td>- Treats all features as independent</td>\n",
    " <td>- Text data</td></tr>\n",
    "\n",
    "<tr><td>k-NN</td>\n",
    " <td>Creates a **cluster** of the $k$-closest records and combines their labels (e.g., with majority voting or an average).</td>\n",
    " <td>- Works with any number of labels<br />\n",
    "     - Very fast \"learning\" (lazy)</td>\n",
    " <td>- Very slow prediction</td>\n",
    " <td>- When choosing closest cases makes sense to the users/stakeholders</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Unsupervised\n",
    "Unsupervised algorithms are use for data where there is no target variable or no labels for your target variable. This semester we covered two algorithms (and will cover a third one in the second half of class).\n",
    "\n",
    "<table>\n",
    "<tr><td>Model</td>\n",
    "<td>Overview</td>\n",
    "<td>Pros</td>\n",
    "<td>Cons</td></tr>\n",
    "\n",
    "<tr><td>K-Means</td>\n",
    " <td>Creates **$k$ clusters** where each record belongs to the cluster with the closest mean (center)</td>\n",
    " <td></td>\n",
    " <td>- k is very likely unknown<br />\n",
    "     - Nondeterministic</td></tr>\n",
    " \n",
    "<tr><td>Hierarchical Clustering</td>\n",
    " <td>Creates an increasing number of clusters by continually **merging clusters** that are closest together (clusters can be single records)</td>\n",
    " <td>- The number of clusters does not need to be preset</td>\n",
    " <td></td></tr>\n",
    "\n",
    "<!--<tr><td>Dimensionality Reduction</td>\n",
    " <td>Takes a set of records with $M$ features and reduces it to the top $m$ features (that explain the most variance), where $m < M$.</td>\n",
    " <td></td>\n",
    " <td></td></tr>-->\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementation\n",
    "All of these algorithms have an implementation in sklearn. Some algorithms, like SVM, have multiple implementations. Let's import one implementation of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given you have imported your model, the general process for using the model is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X, Y) # there is no equal sign here!\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't matter what model you are using, it is always the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, Y) # there is no equal sign here!\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assessing\n",
    "How do we know if our model is any good? There are many ways of doing this! It depends on your particular use case.  Here are the two metrics we talked about most for classification/ranking. \n",
    "\n",
    "<table>\n",
    "<tr><td>Metric</td>\n",
    " <td>Overview</td>\n",
    " <td>Pros</td>\n",
    " <td>Cons</td></tr>\n",
    "<tr><td>Accuracy</td>\n",
    " <td>The percentage of things you got correct.</td>\n",
    " <td>- Easy to calculate and interpret</td>\n",
    " <td>- Doesn't account for business costs<br />\n",
    " - Doesn't account for baseline</td></tr>\n",
    "<tr><td>ROC/AUC</td>\n",
    " <td>False positive rate vs. True positive rate.</td>\n",
    " <td>- Allows for fine grained assessment<br />\n",
    " - Deals with skew</td>\n",
    " <td>- Can be difficult to understand<br />\n",
    " - Exploring multiple ROC curves can become messy</td></tr>\n",
    "</table>\n",
    "\n",
    "Accuracy, ROC curves, and area under the ROC curve calculations are straight forward in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "prediction = model.predict(X)\n",
    "probabilities = model.predict_proba(X)\n",
    "\n",
    "print \"The accuracy is %.3f\" % accuracy_score(Y, prediction)\n",
    "print \"The AUC is %.3f\" % roc_auc_score(Y, probabilities[:, 1])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y, probabilities[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Assessing II (splitting)\n",
    "What are the downsides so training and predicting on the same data? Overfitting! We know two ways to work around this: (1) train/test splitting, and (2) cross validation. Both of these things are, again, built into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "prediction = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "\n",
    "print \"The accuracy is %.3f\" % accuracy_score(Y_test, prediction)\n",
    "print \"The AUC is %.3f\" % roc_auc_score(Y_test, probabilities[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "model = LogisticRegression()\n",
    "\n",
    "print \"The accuracy is %.3f\" % np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"accuracy\"))\n",
    "print \"The AUC is %.3f\" % np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"roc_auc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tuning and Complexity\n",
    "By default, all the models we use in sklearn have some settings that manage how complex they are. We've learned quite a few of these complexity parameters (usually called hyper parameters) already. The act of finding the \"best\" parameter is usually done through the act of \"hyper parameter tuning\".\n",
    "\n",
    "<table>\n",
    "<tr><td>Model Type</td>\n",
    " <td>Parameter</td>\n",
    " <td>Explanation</td>\n",
    " <td>Good Range</td></tr>\n",
    "\n",
    "<tr><td>Tree</td>\n",
    " <td>max_depth</td>\n",
    " <td>Maximum number of levels to build</td>\n",
    " <td>[1, log<sub>2</sub>(# records)]</td></tr>\n",
    "\n",
    "<tr><td>Tree</td>\n",
    " <td>min_samples_split</td>\n",
    " <td>Minimum number of records that must be in a node for it to be split.</td>\n",
    " <td>[1, # records]</td></tr>\n",
    " \n",
    "<tr><td>Tree</td>\n",
    " <td>min_samples_leaf</td>\n",
    " <td>Minimum number of records that must be at a node to call it a leaf.</td>\n",
    " <td>[1, # records]</td></tr>\n",
    "\n",
    "<tr><td>LR</td>\n",
    " <td>C</td>\n",
    " <td>Regularization parameter. How heavily should the model be penalized for being complex?</td>\n",
    " <td>[10<sup>-10</sup>, 10<sup>10</sup>]</td></tr>\n",
    "\n",
    "<tr><td>SVM</td>\n",
    " <td>C</td>\n",
    " <td>Similar to logistic regression</td>\n",
    " <td>[10<sup>-10</sup>, 10<sup>10</sup>]</td></tr>\n",
    " \n",
    "<tr><td>NB</td>\n",
    " <td>alpha</td>\n",
    " <td>Smoothing constant. Essential to ensure 0 probabilities don't zero-out the product.  Used also to keep small counts from making a big difference.</td>\n",
    " <td>[0, ...]</td></tr>\n",
    " \n",
    "<tr><td>k-NN</td>\n",
    " <td>k</td>\n",
    " <td>Number of neighbors, usually odd to avoid ties</td>\n",
    " <td>[1, number_records]</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hyper_parameters = range(-10, 11)\n",
    "accuracies = []\n",
    "aucs = []\n",
    "\n",
    "for hyper_parameter in hyper_parameters:\n",
    "    c = np.power(10.0, hyper_parameter)\n",
    "    \n",
    "    model = LogisticRegression(C=c)\n",
    "    \n",
    "    accuracies.append(np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"accuracy\")))\n",
    "    aucs.append(np.mean(cross_val_score(model, X, Y, cv=5, scoring=\"roc_auc\")))\n",
    "\n",
    "print \"Maximum accuracy is %.3f and occured with parameter setting of %.3e\" % (np.max(accuracies), hyper_parameters[np.argmax(accuracies)])\n",
    "\n",
    "print \"Maximum AUC is %.3f and occured with parameter setting of %.3e\" % (np.max(aucs), hyper_parameters[np.argmax(aucs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
